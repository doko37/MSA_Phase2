{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Section - Phase 3\n",
    "# Author: Peter Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "batch1 = unpickle('./data/data_batch_1')\n",
    "batch2 = unpickle('./data/data_batch_2')\n",
    "batch3 = unpickle('./data/data_batch_3')\n",
    "batch4 = unpickle('./data/data_batch_4')\n",
    "batch5 = unpickle('./data/data_batch_5')\n",
    "test = unpickle('./data/test_batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Exploratory Data Analaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3072)\n",
      "(2000, 3072)\n",
      "(2000, 3072)\n",
      "(2000, 3072)\n",
      "(2000, 3072)\n",
      "(2000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# Each batch contains 10000 images with each image storing a 32x32 colour image. The first 1024 contains the red channel values, the next 1024 contains the green, and the final contains the blue.\n",
    "# Therefore each batch should have a shape of 10000x3072. I will only take the first 2000 images from each batch to have a total of 10000 images for the training batch. Since the images are ordered randomly for each batch\n",
    "# there should not be any large imbalances between the classes.\n",
    "\n",
    "train1_x = batch1[b'data'][:2000]\n",
    "# train1_x = train1_x[:2000]\n",
    "print(train1_x.shape)\n",
    "train1_y = batch1[b'labels'][:2000]\n",
    "\n",
    "train2_x = batch2[b'data'][:2000]\n",
    "print(train2_x.shape)\n",
    "train2_y = batch2[b'labels'][:2000]\n",
    "\n",
    "train3_x = batch3[b'data'][:2000]\n",
    "print(train3_x.shape)\n",
    "train3_y = batch3[b'labels'][:2000]\n",
    "\n",
    "train4_x = batch4[b'data'][:2000]\n",
    "print(train4_x.shape)\n",
    "train4_y = batch4[b'labels'][:2000]\n",
    "\n",
    "train5_x = batch5[b'data'][:2000]\n",
    "print(train5_x.shape)\n",
    "train5_y = batch5[b'labels'][:2000]\n",
    "\n",
    "# Combining all the batches into one list. Since each array will be concatenated in order, the indeces for labels with the corresponding images will stay the same.\n",
    "\n",
    "train_x = np.concatenate((train1_x, train2_x, train3_x, train4_x, train5_x))\n",
    "train_y = np.concatenate((train1_y, train2_y, train3_y, train4_y, train5_y))\n",
    "\n",
    "test_x = test[b'data'][:2000]\n",
    "print(test_x.shape)\n",
    "test_y = test[b'labels'][:2000]\n",
    "\n",
    "# Now we have all the required data and labels to be able to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n",
      "(10000,)\n",
      "[255 253 253 ...  83  83  84]\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(train_x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "#Producing the first 5 images from the training batch.\n",
    "\n",
    "for numImg in range(5):\n",
    "    w, h = 32, 32\n",
    "    data = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for i in range(32):\n",
    "        for j in range(32):\n",
    "            data[i][j] = [train_x[numImg][i * 32 + j], train_x[numImg][i * 32 + j + 1024], train_x[numImg][i * 32 + j + 2048]]\n",
    "\n",
    "    img = Image.fromarray(data, 'RGB')\n",
    "    img.save(f\"img{numImg}.png\")\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label I will be choosing is \"ship\" which is a value of 8.\n",
    "\n",
    "# 3. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of filtered training data: 1045\n",
      "Length of filtered testing data: 217\n"
     ]
    }
   ],
   "source": [
    "filter_train_x = []\n",
    "filter_train_y = []\n",
    "filter_test_x = []\n",
    "filter_test_y = []\n",
    "\n",
    "for i in range(len(train_y)):\n",
    "    if(train_y[i] == 8):\n",
    "        filter_train_x.append(train_x[i])\n",
    "        filter_train_y.append(train_y[i])\n",
    "\n",
    "for i in range(len(test_y)):\n",
    "    if(test_y[i] == 8):\n",
    "        filter_test_x.append(test_x[i])\n",
    "        filter_test_y.append(test_y[i])\n",
    "\n",
    "print(f\"Length of filtered training data: {len(filter_train_y)}\")\n",
    "print(f\"Length of filtered testing data: {len(filter_test_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurances for each class in test batch: [196, 198, 195, 199, 198, 185, 216, 193, 217, 203]\n",
      "Number of occurances for each class in train batch: [981, 1003, 994, 982, 1033, 969, 999, 988, 1045, 1006]\n"
     ]
    }
   ],
   "source": [
    "counter = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for i in range(len(test_y)):\n",
    "    counter[test_y[i]] += 1\n",
    "\n",
    "print(f\"Number of occurances for each class in test batch: {counter}\")\n",
    "\n",
    "counter = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for i in range(len(train_y)):\n",
    "    counter[train_y[i]] += 1\n",
    "\n",
    "print(f\"Number of occurances for each class in train batch: {counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are no large imbalances in the number of occuring classes in both the training and testing batch.\n",
    "\n",
    "Currently, the labels for each class are integer values from 0-9, however to use these values with a NN we need to convert these values into a more useable form as node in a NN can only output a zero or a one. To be able to classify each class individual with 10 output nodes, I will use a technique called \"One hot encoding\" where each numeric value is converted in an array where all the values are zero except for the corresponding index. For example, a value of 3 will be converted to: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. The 4th (starting from 0) index is 1 while every other index is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohn_test_y = np.array([[int(i == label_indx) for i in range(10)] for label_indx in test_y])\n",
    "\n",
    "ohn_filter_train_y = np.array([[int(i == label_indx) for i in range(10)] for label_indx in filter_train_y])\n",
    "ohn_filtered_test_y = np.array([[int(i == label_indx) for i in range(10)] for label_indx in filter_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9 9 ... 5 3 2]\n",
      "[3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9, 5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9, 7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6, 8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6, 6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7, 8, 3, 1, 2, 8, 0, 8, 3, 5, 2, 4, 1, 8, 9, 1, 2, 9, 7, 2, 9, 6, 5, 6, 3, 8, 7, 6, 2, 5, 2, 8, 9, 6, 0, 0, 5, 2, 9, 5, 4, 2, 1, 6, 6, 8, 4, 8, 4, 5, 0, 9, 9, 9, 8, 9, 9, 3, 7, 5, 0, 0, 5, 2, 2, 3, 8, 6, 3, 4, 0, 5, 8, 0, 1, 7, 2, 8, 8, 7, 8, 5, 1, 8, 7, 1, 3, 0, 5, 7, 9, 7, 4, 5, 9, 8, 0, 7, 9, 8, 2, 7, 6, 9, 4, 3, 9, 6, 4, 7, 6, 5, 1, 5, 8, 8, 0, 4, 0, 5, 5, 1, 1, 8, 9, 0, 3, 1, 9, 2, 2, 5, 3, 9, 9, 4, 0, 3, 0, 0, 9, 8, 1, 5, 7, 0, 8, 2, 4, 7, 0, 2, 3, 6, 3, 8, 5, 0, 3, 4, 3, 9, 0, 6, 1, 0, 9, 1, 0, 7, 9, 1, 2, 6, 9, 3, 4, 6, 0, 0, 6, 6, 6, 3, 2, 6, 1, 8, 2, 1, 6, 8, 6, 8, 0, 4, 0, 7, 7, 5, 5, 3, 5, 2, 3, 4, 1, 7, 5, 4, 6, 1, 9, 3, 6, 6, 9, 3, 8, 0, 7, 2, 6, 2, 5, 8, 5, 4, 6, 8, 9, 9, 1, 0, 2, 2, 7, 3, 2, 8, 0, 9, 5, 8, 1, 9, 4, 1, 3, 8, 1, 4, 7, 9, 4, 2, 7, 0, 7, 0, 6, 6, 9, 0, 9, 2, 8, 7, 2, 2, 5, 1, 2, 6, 2, 9, 6, 2, 3, 0, 3, 9, 8, 7, 8, 8, 4, 0, 1, 8, 2, 7, 9, 3, 6, 1, 9, 0, 7, 3, 7, 4, 5, 0, 0, 2, 9, 3, 4, 0, 6, 2, 5, 3, 7, 3, 7, 2, 5, 3, 1, 1, 4, 9, 9, 5, 7, 5, 0, 2, 2, 2, 9, 7, 3, 9, 4, 3, 5, 4, 6, 5, 6, 1, 4, 3, 4, 4, 3, 7, 8, 3, 7, 8, 0, 5, 7, 6, 0, 5, 4, 8, 6, 8, 5, 5, 9, 9, 9, 5, 0, 1, 0, 8, 1, 1, 8, 0, 2, 2, 0, 4, 6, 5, 4, 9, 4, 7, 9, 9, 4, 5, 6, 6, 1, 5, 3, 8, 9, 5, 8, 5, 7, 0, 7, 0, 5, 0, 0, 4, 6, 9, 0, 9, 5, 6, 6, 6, 2, 9, 0, 1, 7, 6, 7, 5, 9, 1, 6, 2, 5, 5, 5, 8, 5, 9, 4, 6, 4, 3, 2, 0, 7, 6, 2, 2, 3, 9, 7, 9, 2, 6, 7, 1, 3, 6, 6, 8, 9, 7, 5, 4, 0, 8, 4, 0, 9, 3, 4, 8, 9, 6, 9, 2, 6, 1, 4, 7, 3, 5, 3, 8, 5, 0, 2, 1, 6, 4, 3, 3, 9, 6, 9, 8, 8, 5, 8, 6, 6, 2, 1, 7, 7, 1, 2, 7, 9, 9, 4, 4, 1, 2, 5, 6, 8, 7, 6, 8, 3, 0, 5, 5, 3, 0, 7, 9, 1, 3, 4, 4, 5, 3, 9, 5, 6, 9, 2, 1, 1, 4, 1, 9, 4, 7, 6, 3, 8, 9, 0, 1, 3, 6, 3, 6, 3, 2, 0, 3, 1, 0, 5, 9, 6, 4, 8, 9, 6, 9, 6, 3, 0, 3, 2, 2, 7, 8, 3, 8, 2, 7, 5, 7, 2, 4, 8, 7, 4, 2, 9, 8, 8, 6, 8, 8, 7, 4, 3, 3, 8, 4, 9, 4, 8, 8, 1, 8, 2, 1, 3, 6, 5, 4, 2, 7, 9, 9, 4, 1, 4, 1, 3, 2, 7, 0, 7, 9, 7, 6, 6, 2, 5, 9, 2, 9, 1, 2, 2, 6, 8, 2, 1, 3, 6, 6, 0, 1, 2, 7, 0, 5, 4, 6, 1, 6, 4, 0, 2, 2, 6, 0, 5, 9, 1, 7, 6, 7, 0, 3, 9, 6, 8, 3, 0, 3, 4, 7, 7, 1, 4, 7, 2, 7, 1, 4, 7, 4, 4, 8, 4, 7, 7, 5, 3, 7, 2, 0, 8, 9, 5, 8, 3, 6, 2, 0, 8, 7, 3, 7, 6, 5, 3, 1, 3, 2, 2, 5, 4, 1, 2, 9, 2, 7, 0, 7, 2, 1, 3, 2, 0, 2, 4, 7, 9, 8, 9, 0, 7, 7, 0, 7, 8, 4, 6, 3, 3, 0, 1, 3, 7, 0, 1, 3, 1, 4, 2, 3, 8, 4, 2, 3, 7, 8, 4, 3, 0, 9, 0, 0, 1, 0, 4, 4, 6, 7, 6, 1, 1, 3, 7, 3, 5, 2, 6, 6, 5, 8, 7, 1, 6, 8, 8, 5, 3, 0, 4, 0, 1, 3, 8, 8, 0, 6, 9, 9, 9, 5, 5, 8, 6, 0, 0, 4, 2, 3, 2, 7, 2, 2, 5, 9, 8, 9, 1, 7, 4, 0, 3, 0, 1, 3, 8, 3, 9, 6, 1, 4, 7, 0, 3, 7, 8, 9, 1, 1, 6, 6, 6, 6, 9, 1, 9, 9, 4, 2, 1, 7, 0, 6, 8, 1, 9, 2, 9, 0, 4, 7, 8, 3, 1, 2, 0, 1, 5, 8, 4, 6, 3, 8, 1, 3, 8, 5, 0, 8, 4, 8, 1, 1, 8, 9, 6, 0, 8, 6, 1, 3, 4, 1, 6, 0, 5, 1, 1, 0, 0, 3, 5, 0, 0, 6, 6, 3, 3, 6, 3, 6, 6, 0, 7, 2, 2, 7, 5, 5, 2, 8, 5, 2, 1, 1, 4, 3, 2, 0, 3, 1, 5, 3, 7, 6, 8, 9, 1, 6, 4, 9, 3, 9, 0, 9, 6, 3, 6, 0, 7, 3, 8, 0, 0, 0, 6, 6, 6, 9, 2, 5, 4, 4, 6, 3, 6, 0, 8, 6, 0, 6, 2, 7, 5, 1, 2, 7, 8, 8, 0, 9, 4, 9, 7, 2, 0, 2, 8, 3, 8, 9, 1, 5, 5, 4, 7, 5, 3, 8, 3, 3, 6, 2, 8, 4, 3, 7, 1, 2, 4, 1, 6, 9, 0, 5, 8, 6, 1, 8, 6, 1, 4, 2, 6, 2, 7, 2, 2, 0, 8, 6, 9, 1, 7, 1, 8, 8, 0, 7, 3, 8, 0, 3, 4, 3, 7, 7, 9, 2, 3, 1, 9, 1, 9, 6, 3, 3, 3, 1, 0, 6, 1, 4, 1, 0, 0, 1, 1, 6, 5, 4, 6, 2, 0, 7, 9, 8, 7, 2, 0, 6, 8, 1, 4, 3, 7, 0, 6, 1, 8, 5, 7, 8, 4, 8, 3, 9, 9, 9, 8, 7, 6, 6, 3, 5, 1, 5, 9, 1, 4, 1, 5, 7, 0, 1, 5, 2, 0, 8, 8, 5, 6, 7, 3, 2, 4, 7, 2, 5, 8, 2, 4, 9, 2, 1, 8, 1, 9, 8, 8, 8, 9, 0, 4, 3, 3, 1, 8, 4, 6, 3, 3, 5, 2, 2, 8, 3, 8, 9, 5, 8, 9, 8, 9, 1, 6, 5, 9, 4, 4, 8, 0, 7, 2, 9, 7, 4, 1, 6, 4, 4, 9, 1, 2, 5, 6, 0, 8, 6, 1, 9, 4, 5, 9, 5, 0, 7, 2, 0, 0, 4, 2, 6, 6, 5, 5, 2, 8, 1, 7, 3, 1, 4, 5, 6, 5, 1, 4, 7, 0, 9, 4, 3, 8, 2, 8, 4, 7, 2, 3, 1, 5, 2, 9, 8, 9, 7, 9, 5, 1, 4, 0, 8, 2, 3, 8, 9, 1, 1, 3, 2, 4, 9, 3, 1, 7, 4, 6, 2, 8, 9, 5, 3, 9, 5, 5, 6, 7, 2, 4, 6, 3, 1, 0, 7, 2, 5, 4, 7, 6, 1, 1, 9, 8, 1, 0, 1, 3, 1, 1, 1, 7, 3, 9, 6, 8, 4, 6, 8, 4, 9, 4, 7, 9, 7, 6, 8, 4, 9, 7, 0, 1, 6, 1, 5, 9, 0, 4, 3, 4, 1, 3, 0, 8, 4, 6, 2, 2, 6, 5, 3, 6, 2, 1, 1, 8, 6, 0, 4, 0, 1, 9, 7, 1, 3, 7, 7, 8, 7, 7, 3, 9, 7, 7, 7, 2, 1, 2, 8, 6, 4, 0, 7, 9, 8, 6, 8, 4, 9, 1, 7, 2, 2, 8, 5, 8, 1, 2, 2, 4, 1, 2, 5, 2, 8, 1, 8, 1, 8, 6, 0, 2, 4, 1, 3, 6, 7, 7, 4, 4, 3, 3, 4, 5, 2, 4, 3, 7, 8, 4, 4, 4, 5, 4, 3, 2, 8, 4, 5, 5, 4, 1, 4, 2, 5, 1, 6, 4, 3, 4, 4, 0, 8, 8, 4, 5, 7, 5, 6, 9, 1, 6, 7, 2, 0, 1, 4, 5, 6, 0, 0, 2, 7, 5, 6, 0, 6, 2, 9, 1, 7, 7, 5, 2, 5, 6, 4, 1, 4, 3, 3, 3, 0, 3, 5, 5, 8, 9, 7, 3, 1, 3, 3, 3, 4, 4, 2, 3, 3, 8, 1, 7, 7, 0, 7, 4, 5, 1, 4, 2, 4, 3, 9, 9, 4, 9, 9, 1, 8, 1, 6, 7, 5, 5, 4, 9, 7, 6, 5, 9, 2, 4, 0, 7, 8, 5, 5, 0, 0, 9, 9, 8, 2, 5, 4, 8, 3, 6, 3, 6, 0, 6, 6, 6, 9, 6, 6, 8, 6, 2, 4, 5, 8, 1, 2, 7, 6, 5, 7, 8, 1, 8, 0, 8, 6, 9, 2, 8, 9, 4, 0, 9, 4, 9, 5, 7, 5, 5, 9, 5, 3, 0, 1, 9, 7, 2, 4, 1, 0, 8, 0, 3, 1, 7, 0, 0, 4, 8, 6, 2, 4, 0, 0, 9, 0, 8, 4, 5, 9, 3, 9, 0, 5, 6, 5, 0, 1, 4, 8, 1, 0, 5, 2, 1, 0, 2, 8, 1, 5, 6, 7, 7, 2, 6, 2, 5, 0, 1, 4, 2, 5, 4, 6, 2, 2, 1, 7, 2, 8, 5, 5, 3, 0, 4, 8, 3, 7, 6, 3, 8, 1, 0, 1, 3, 3, 0, 7, 4, 9, 5, 3, 6, 0, 1, 4, 4, 4, 4, 2, 2, 5, 8, 1, 5, 9, 8, 1, 1, 5, 3, 9, 9, 7, 6, 5, 0, 8, 4, 7, 0, 9, 2, 8, 4, 7, 1, 3, 9, 6, 8, 9, 0, 4, 9, 6, 7, 8, 9, 4, 8, 9, 7, 2, 5, 3, 7, 1, 0, 2, 9, 5, 5, 8, 5, 4, 2, 8, 3, 5, 5, 7, 7, 8, 6, 2, 8, 2, 3, 5, 6, 8, 0, 2, 3, 7, 0, 1, 9, 1, 3, 7, 5, 8, 3, 2, 9, 6, 8, 6, 9, 3, 8, 9, 8, 0, 7, 8, 5, 0, 0, 1, 3, 9, 1, 5, 3, 4, 4, 0, 9, 9, 9, 9, 8, 2, 4, 2, 2, 5, 1, 9, 1, 0, 9, 4, 2, 1, 6, 0, 3, 7, 6, 3, 1, 8, 6, 5, 7, 2, 8, 4, 4, 8, 3, 5, 0, 5, 7, 4, 4, 2, 2, 7, 3, 6, 0, 2, 7, 6, 2, 3, 0, 7, 7, 8, 1, 1, 4, 6, 0, 6, 6, 5, 5, 6, 3, 9, 3, 6, 8, 7, 6, 4, 9, 5, 6, 4, 1, 6, 3, 8, 2, 3, 9, 8, 5]\n",
      "(10000, 3072)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_y)\n",
    "print(test_y)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.api._v2.keras import layers, Sequential, Input, optimizers, losses\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import datetime\n",
    "\n",
    "# My model was having problems with evaluating labels that are one hot encoded, so I will use a regular 1-D with integers as labels.\n",
    "# This is possible as I am using sparse categorical cross entropy, which allows the model to use regular integers to classify datasets.\n",
    "# The same loss function is used so the model won't behave differently.\n",
    "def getSparceModel():\n",
    "    model = Sequential(name=\"32x32_Image_Classification_Model\")\n",
    "    model.add(layers.Dense(255, input_dim=3072, activation='relu', name='input'))\n",
    "\n",
    "    # Squishing (or normalizing) the RGB values from 0-255 to 0-1.\n",
    "    model.add(layers.Rescaling(scale=1./255, name=\"Normaliser\"))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(255, activation='relu'))\n",
    "    model.add(layers.Dense(255, activation='relu'))\n",
    "    model.add(layers.Dense(255, activation='relu'))\n",
    "    model.add(layers.Dense(255, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(10, activation=\"softmax\", name='output'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.0001), \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy', 'accuracy'],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "batch_size = [650, 750, 850]\n",
    "epochs = [75, 100, 125]\n",
    "\n",
    "param_grid = dict(epochs=epochs, batch_size=batch_size)\n",
    "model = KerasClassifier(build_fn=getSparceModel, verbose=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', n_jobs=1, refit='boolean')\n",
    "grid_result = grid.fit(train_x, train_y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the GridSearch the best result is: Best: 0.454400 using {'batch_size': 750, 'epochs': 125}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#model.save('output/my_model2')\n",
    "ohn_train_y = np.array([[int(i == label_indx) for i in range(10)] for label_indx in train_y])\n",
    "\n",
    "print(ohn_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 3s 10ms/step - loss: 2.0610 - sparse_categorical_accuracy: 0.2462 - accuracy: 0.2462 - val_loss: 1.9019 - val_sparse_categorical_accuracy: 0.3025 - val_accuracy: 0.3025\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 1.8713 - sparse_categorical_accuracy: 0.3276 - accuracy: 0.3276 - val_loss: 1.8012 - val_sparse_categorical_accuracy: 0.3625 - val_accuracy: 0.3625\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 1.7934 - sparse_categorical_accuracy: 0.3530 - accuracy: 0.3530 - val_loss: 1.7576 - val_sparse_categorical_accuracy: 0.3710 - val_accuracy: 0.3710\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 1.7278 - sparse_categorical_accuracy: 0.3778 - accuracy: 0.3778 - val_loss: 1.7434 - val_sparse_categorical_accuracy: 0.3655 - val_accuracy: 0.3655\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 1.6823 - sparse_categorical_accuracy: 0.4001 - accuracy: 0.4001 - val_loss: 1.7122 - val_sparse_categorical_accuracy: 0.3935 - val_accuracy: 0.3935\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "282/282 [==============================] - 2s 8ms/step - loss: 1.6453 - sparse_categorical_accuracy: 0.4078 - accuracy: 0.4078 - val_loss: 1.6969 - val_sparse_categorical_accuracy: 0.4055 - val_accuracy: 0.4055\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "282/282 [==============================] - 2s 8ms/step - loss: 1.6097 - sparse_categorical_accuracy: 0.4282 - accuracy: 0.4282 - val_loss: 1.7661 - val_sparse_categorical_accuracy: 0.3690 - val_accuracy: 0.3690\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 1.5651 - sparse_categorical_accuracy: 0.4468 - accuracy: 0.4468 - val_loss: 1.6107 - val_sparse_categorical_accuracy: 0.4410 - val_accuracy: 0.4410\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 1.5298 - sparse_categorical_accuracy: 0.4600 - accuracy: 0.4600 - val_loss: 1.5939 - val_sparse_categorical_accuracy: 0.4425 - val_accuracy: 0.4425\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 1.5051 - sparse_categorical_accuracy: 0.4699 - accuracy: 0.4699 - val_loss: 1.6128 - val_sparse_categorical_accuracy: 0.4345 - val_accuracy: 0.4345\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 1.4887 - sparse_categorical_accuracy: 0.4709 - accuracy: 0.4709 - val_loss: 1.5642 - val_sparse_categorical_accuracy: 0.4480 - val_accuracy: 0.4480\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 1.4438 - sparse_categorical_accuracy: 0.4887 - accuracy: 0.4887 - val_loss: 1.5932 - val_sparse_categorical_accuracy: 0.4430 - val_accuracy: 0.4430\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 1.4179 - sparse_categorical_accuracy: 0.4934 - accuracy: 0.4934 - val_loss: 1.5735 - val_sparse_categorical_accuracy: 0.4515 - val_accuracy: 0.4515\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 1.3969 - sparse_categorical_accuracy: 0.5019 - accuracy: 0.5019 - val_loss: 1.6422 - val_sparse_categorical_accuracy: 0.4265 - val_accuracy: 0.4265\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 1.3602 - sparse_categorical_accuracy: 0.5132 - accuracy: 0.5132 - val_loss: 1.5871 - val_sparse_categorical_accuracy: 0.4530 - val_accuracy: 0.4530\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "[0.326, 0.351, 0.353, 0.36, 0.402, 0.422, 0.388, 0.448, 0.479, 0.463, 0.518, 0.478, 0.505, 0.488, 0.51]\n",
      "63/63 [==============================] - 0s 2ms/step\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5871 - sparse_categorical_accuracy: 0.4530 - accuracy: 0.4530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5870673656463623, 0.453000009059906, 0.453000009059906]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = getSparceModel()\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=15)\n",
    "\n",
    "sss.get_n_splits(train_x, train_y)\n",
    "\n",
    "acc = []\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "for train_index, test_index in sss.split(train_x, train_y):\n",
    "    X_train, X_test = train_x[train_index], train_x[test_index]\n",
    "    y_train, y_test = train_y[train_index], train_y[test_index]\n",
    "    model.fit(np.array(X_train), np.array(y_train), validation_data=(np.array(test_x), np.array(test_y)), callbacks=[tensorboard_callback])\n",
    "    pred = model.predict(X_test)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    acc.append(accuracy_score(y_test, pred))\n",
    "\n",
    "print(acc)\n",
    "\n",
    "test_pred = model.predict(np.array(test_x))\n",
    "model.evaluate(np.array(test_x), np.array(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217, 3072)\n",
      "(217,)\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5871 - sparse_categorical_accuracy: 0.4530 - accuracy: 0.4530\n",
      "Evaluation will every image from test batch: [1.5870673656463623, 0.453000009059906, 0.453000009059906]\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.0937 - sparse_categorical_accuracy: 0.6912 - accuracy: 0.6912\n",
      "Evaluation will every image from filtered test batch: [1.0936577320098877, 0.6912442445755005, 0.6912442445755005]\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "[9 8 8 8 4 8 8 8 8 8 8 8 8 1 8 0 8 9 9 8 9 8 8 8 3 8 2 5 8 0 0 4 8 7 5 8 9\n",
      " 2 8 0 8 8 8 1 8 0 0 3 8 8 8 8 8 9 4 0 0 8 8 8 8 8 8 8 8 8 5 8 9 8 5 8 8 2\n",
      " 8 8 8 8 9 8 8 8 8 8 8 8 4 8 8 8 8 8 8 8 8 8 8 3 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 1 8 8 8 9 8 8 0 8 8 0 0 8 0 8 5 1 8 8 0 8 8 8 8 8 1 2 8 0 7\n",
      " 8 8 8 8 3 8 8 8 8 8 8 8 8 8 8 8 8 8 4 8 8 8 7 8 9 4 3 9 8 8 3 8 8 6 1 8 1\n",
      " 1 8 8 8 0 8 8 8 8 1 8 8 9 8 8 8 8 8 0 2 8 0 8 8 8 8 8 8 8 2 6 3]\n"
     ]
    }
   ],
   "source": [
    "test_y = np.array(test_y)\n",
    "filter_test_y = np.array(filter_test_y)\n",
    "filter_test_x = np.array(filter_test_x)\n",
    "\n",
    "\n",
    "print(filter_test_x.shape)\n",
    "print(filter_test_y.shape)\n",
    "\n",
    "print(f\"Evaluation will every image from test batch: {model.evaluate(test_x, test_y)}\")\n",
    "\n",
    "print(f\"Evaluation will every image from filtered test batch: {model.evaluate(filter_test_x, filter_test_y, callbacks=tensorboard_callback)}\")\n",
    "\n",
    "pred = model.predict(filter_test_x)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the overall accuracy of the model is around 45% which is quite low, however when testing the model with the filtered test batch which only contains the images of the class ship, the accuracy is much better at around 75%.\n",
    "\n",
    "### Function that takes any image, resizes it to 32 x 32 and then evaluates it using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./output/mymodel\\assets\n",
      "(1, 3072)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Model prediction: [4], Actual class: [8]\n"
     ]
    }
   ],
   "source": [
    "model.save('./output/mymodel')\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def predictImg(img, model, y):\n",
    "    newImg = cv2.imread(img)\n",
    "\n",
    "    newImg = cv2.resize(newImg, (32, 32))\n",
    "\n",
    "    odimg = np.reshape(newImg, (3072,))\n",
    "\n",
    "    arr = np.array([odimg])\n",
    "\n",
    "    print(arr.shape)\n",
    "\n",
    "    pred = model.predict(arr)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \n",
    "    print(f\"Model prediction: {pred}, Actual class: {y}\")\n",
    "\n",
    "    return newImg\n",
    "\n",
    "y = np.array([8])\n",
    "\n",
    "img = predictImg('./images/ship.jpg', model=model, y=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9f1f59101e07bffb7c2ecfaca1a3c7ffe3cd326ee75e914ab1b038684b38c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
